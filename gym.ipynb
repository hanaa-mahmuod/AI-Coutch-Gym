{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'data'\n",
    "model_save_path = 'The saved model/model.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path):\n",
    "    x = []\n",
    "    y = []\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "    # Loop through dataset directory\n",
    "    for exercise_folder in os.listdir(dataset_path):\n",
    "        exercise_label = exercise_folder\n",
    "        exercise_folder_path = os.path.join(dataset_path, exercise_folder)  \n",
    "\n",
    "        # Loop through video files in exercise folder\n",
    "        for video_file in os.listdir(exercise_folder_path):\n",
    "            video_path = os.path.join(exercise_folder_path, video_file)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            while cap.isOpened():\n",
    "                success, image = cap.read()\n",
    "                if not success:\n",
    "                    break\n",
    "                # Process image using MediaPipe Pose Detection\n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                result = pose.process(image_rgb)\n",
    "                if result.pose_landmarks:\n",
    "                    # Extract pose landmarks\n",
    "                    landmarks = [[lm.x, lm.y] for lm in result.pose_landmarks.landmark]\n",
    "                    x.append(landmarks)\n",
    "                    y.append(exercise_label)\n",
    "            cap.release()\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numerical values\n",
    "label_map = {\n",
    "    'deadlift_False': 0,\n",
    "    'deadlift_True': 1,\n",
    "    'lat_pulldown_False': 2,\n",
    "    'lat_pulldown_True': 3\n",
    "}\n",
    "y = np.array([label_map[label] for label in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x.dtype != np.float32:\n",
    "    x = x.astype(np.float32)\n",
    "if y.dtype != np.float32:\n",
    "    y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import  BatchNormalization\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    \n",
    "    # Convolutional layers\n",
    "    tf.keras.layers.Conv1D(64, kernel_size=3, activation='tanh', padding='same'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    \n",
    "    tf.keras.layers.Conv1D(128, kernel_size=3, activation='tanh', padding='same'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    \n",
    "    tf.keras.layers.Conv1D(256, kernel_size=3, activation='tanh', padding='same'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    \n",
    "    # Flatten layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # Dense layers\n",
    "    tf.keras.layers.Dense(512, activation='tanh'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False)\n",
    "# Callback for early stopping\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y_train,epochs=150,batch_size=64,validation_split=0.2,callbacks=[cp_callback, es_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on training data\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Evaluate model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict labels for test data\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "# Error Analysis\n",
    "misclassifications = np.where(y_test != y_pred)[0]\n",
    "print(\"Misclassified Instances:\", misclassifications)\n",
    "\n",
    "# Result Analysis\n",
    "accuracy = np.mean(y_test == y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Visualization for Test and Train (Learning curves)\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = [\n",
    "    'deadlift-4.mp4',\n",
    "    'deadlift_false.mp4',\n",
    "    'false lat pulldown.mp4',\n",
    "    'lat pulldown_35.mp4'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_path in video_paths:\n",
    "     \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if the video/camera opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Unable to open video file or camera.\")\n",
    "        exit()\n",
    "\n",
    "    # Initialize MediaPipe Pose Detection\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "    # Function to resize frame while maintaining aspect ratio\n",
    "    def resize_frame(frame, width=None, height=None):\n",
    "        h, w = frame.shape[:2]\n",
    "        if width is None and height is None:\n",
    "            return frame\n",
    "        if width is None:\n",
    "            ratio = height / h\n",
    "        else:\n",
    "            ratio = width / w\n",
    "        dim = (int(w * ratio), int(h * ratio))\n",
    "        return cv2.resize(frame, dim, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "    \n",
    "        # Resize the frame for faster processing (adjust the dimensions as needed)\n",
    "        resized_frame = resize_frame(frame, width=440)  # Adjust width as needed\n",
    "    \n",
    "        # Process frame with MediaPipe Pose Detection\n",
    "        frame_rgb = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "        result = pose.process(frame_rgb)\n",
    "    \n",
    "        # Check if pose landmarks are detected\n",
    "        if result.pose_landmarks:\n",
    "            # Extract pose landmarks\n",
    "            landmarks = [[lm.x, lm.y] for lm in result.pose_landmarks.landmark]\n",
    "\n",
    "            # Preprocess landmarks (reshape, convert to numpy array, etc.)\n",
    "            # Example:\n",
    "            landmarks_array = np.array(landmarks, dtype=np.float32)\n",
    "            landmarks_array = landmarks_array[np.newaxis, ...]  # Add batch dimension\n",
    "\n",
    "            # Use the model to make predictions\n",
    "            predictions = model.predict(landmarks_array)\n",
    "            predicted_class = \"not_an_exercise\"\n",
    "            # Example: Print the predicted class\n",
    "            min_value = 0.5\n",
    "            if np.max(predictions) >= min_value:\n",
    "                predicted_class = \"\"\n",
    "                if np.argmax(predictions) == 0:\n",
    "                    predicted_class = \"deadlift_False\"\n",
    "                    m = 0\n",
    "                elif np.argmax(predictions) == 1:\n",
    "                    predicted_class = \"deadlift_True\"\n",
    "                    m = 1\n",
    "                elif np.argmax(predictions) == 2:\n",
    "                    predicted_class = \"lat_pulldown_False\"\n",
    "                    m = 0\n",
    "                elif np.argmax(predictions) == 3:\n",
    "                    predicted_class = \"lat_pulldown_True\"\n",
    "                    m = 1                        \n",
    "            else:\n",
    "                predicted_class = \"not_an_exercise\"  \n",
    "            if m == 0:     \n",
    "               cv2.putText(resized_frame, predicted_class, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            elif m == 1:\n",
    "               cv2.putText(resized_frame, predicted_class, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)  \n",
    "         \n",
    "\n",
    "\n",
    "        # Display the resized frame\n",
    "        cv2.imshow(\"output\", resized_frame)\n",
    "    \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the video capture object and close all windows\n",
    "    cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the video/camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video file or camera.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize MediaPipe Pose Detection\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Function to resize frame while maintaining aspect ratio\n",
    "def resize_frame(frame, width=None, height=None):\n",
    "    h, w = frame.shape[:2]\n",
    "    if width is None and height is None:\n",
    "        return frame\n",
    "    if width is None:\n",
    "        ratio = height / h\n",
    "    else:\n",
    "        ratio = width / w\n",
    "    dim = (int(w * ratio), int(h * ratio))\n",
    "    return cv2.resize(frame, dim, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Resize the frame for faster processing (adjust the dimensions as needed)\n",
    "    resized_frame = resize_frame(frame, width=640)  # Adjust width as needed\n",
    "    \n",
    "    # Process frame with MediaPipe Pose Detection\n",
    "    frame_rgb = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(frame_rgb)\n",
    "    \n",
    "    # Check if pose landmarks are detected\n",
    "    if result.pose_landmarks:\n",
    "        # Extract pose landmarks\n",
    "        landmarks = [[lm.x, lm.y] for lm in result.pose_landmarks.landmark]\n",
    "\n",
    "        # Preprocess landmarks (reshape, convert to numpy array, etc.)\n",
    "        # Example:\n",
    "        landmarks_array = np.array(landmarks, dtype=np.float32)\n",
    "        landmarks_array = landmarks_array[np.newaxis, ...]  # Add batch dimension\n",
    "\n",
    "        # Use the model to make predictions\n",
    "        predictions = model.predict(landmarks_array)\n",
    "        predicted_class = \"not_an_exercise\"\n",
    "        # Example: Print the predicted class\n",
    "        min_value = 0.5\n",
    "        if np.max(predictions) >= min_value:\n",
    "            predicted_class = \"\"\n",
    "            if np.argmax(predictions) == 0:\n",
    "                predicted_class = \"deadlift_False\"\n",
    "                m = 0\n",
    "            elif np.argmax(predictions) == 1:\n",
    "                predicted_class = \"deadlift_True\"\n",
    "                m = 1\n",
    "            elif np.argmax(predictions) == 2:\n",
    "                predicted_class = \"lat_pulldown_False\"\n",
    "                m = 0\n",
    "            elif np.argmax(predictions) == 3:\n",
    "                predicted_class = \"lat_pulldown_True\"\n",
    "                m = 1                        \n",
    "        else:\n",
    "            predicted_class = \"not_an_exercise\"  \n",
    "        if m == 0:     \n",
    "          cv2.putText(resized_frame, predicted_class, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        else:\n",
    "          cv2.putText(resized_frame, predicted_class, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "\n",
    "    # Display the resized frame\n",
    "    cv2.imshow(\"output\", resized_frame)\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict labels for test data\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "# Error Analysis\n",
    "misclassifications = np.where(y_test != y_pred)[0]\n",
    "print(\"Misclassified Instances:\", misclassifications)\n",
    "\n",
    "# Result Analysis\n",
    "accuracy = np.mean(y_test == y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Visualization for Test and Train (Learning curves)\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
